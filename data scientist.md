# What is Data science？

- The study of data to understand the world

  **使用数据来了解我们的世界**

- An art of uncovering insights and trends

  **一门隐藏在数据背后的见解和趋势的艺术**

- Data access drives new insights 

  **增强的计算能力通过分析推动了新的见解和知识**

- The needed computing power to analyse it

  **拥有分析这些数据以揭示新知识所需的计算能力**

****

## Data Scientist's role 

`Data -> Stories -> Insights`

- Transforming data into stories to produce insights, which can help with company's strategies.

  **将数据转化为故事来产生见解来帮助公司或机构指定战略决策**

- Handling unstructured and structured data

  **处理结构化和非结构化的数据**

- Collect insights from data's process

  **从数据中收集见解的过程**

  - Clarifying the problem

    **澄清问题**

  - Data collection

    **数据收集**

  - Analysis

    **数据分析**

  - Recognition

    **模式识别**

  - Storytelling

    **讲故事**

  - Visualization

    **可视化**

****

### **Qualities of a data scientist** (**vital **)

- Curiosity

  **帮助我们探索数据并提出有意义的问题**

-  Sound Argumentation

  **合理的论据可以帮助我们从数据中学习后解释我们的发现，迫使听众根据新信息调整自己的想法**

- Good Judgement

  **良好的判断力会引导我们朝着正确的方向开始**

****

### **Skilled data scientist**

- Versatility

  **多才多艺**

- know a lot about Subject area knowledge

  **拥有特定领域的专业知识**

- Have some experience programming and analysing data and good at communication

  **拥有编程和分析数据的一定经验**

- Comfortable with math

- Curious

- Good storyteller

- From diverse background

- Adept at selecting suitable tools

  **为自己的行业选择适合的工具**

- Apply expertise to problem-solving

  **精通工具之后运用专业知识来解决问题**

****

### **Data scientist ever-evolving field**

- The role will continue to evolve

- May require certifications

- Always need to think logically, algorithmically and methodically

  **始终需要逻辑思考，使用算法并遵循有条不紊的方法**

- Gather data correctly 

  **收集正确的数据**

- Carefully analyse

  **仔细分析**

****

## Defining data science lesson glossary 定义数据科学词汇表

| Term                  | Definition                                                   | Video where the term is introduced |
| --------------------- | ------------------------------------------------------------ | ---------------------------------- |
| Algorithms            | A set of step-by-step instructions to solve a problem or complete a task. | What is Data Science?              |
| Model                 | A representation of the relationships and patterns found in data to make predictions or analyse complex systems retaining essential elements needed for analysis. | What is Data Science?              |
| Outliers              | When a data point or points occur significantly outside of most of the other data in a data set, potentially indicating anomalies, errors, or unique phenomena that could impact statistical analysis or modelling. | What is Data Science?              |
| Quantitative analysis | A systematic approach using mathematical and statistical analysis is used to interpret numerical data. | Many Paths to Data Science         |
| Structured data       | Data is organized and formatted into a predictable schema, usually related tables with rows and columns. | What is Data Science?              |
| Unstructured data     | Unorganized data that lacks a predefined data model or organization makes it harder to analyse using traditional methods. This data type often includes text, images, videos, and other content that doesn’t fit neatly into rows and columns like structured data. | What is Data Science?              |



****

# Different types of file



## Standard file formats

### Delimited text file formats `.CSV` 

- Used to store data as text

  **用于将数据存储为文本**

- Each value is separated by a delimiter

  **每行都用分隔符分隔的值**

- Delimiter - A sequence of one or more characters for specifying the boundary between **independent entities** or **values**

  **分隔符是由一个序列的一个或多个字符用于指定独立实体或值之间的边界**

- Common Delimited:

  - **Comma**  CSV (Comma seperated values)
  - **Tab** TSV  subsitute for CSV 
  - Colon
  - Vertical Bar
  - Space

**分隔符还代表在数据流中指定边界的各种方法之一**

****

### Microsoft Excel Open `.XML` Spreadsheet `.XLSX`

- A Microsoft Excel Open XML file format that falls under spreadsheet file format

  **微软 Excel Open XML 电子表格（XLSX）是一种微软 Excel Open XML 文件格式 ，属于电子表格文件格式**

-  XML-based file format created by Microsoft

  **它是微软创建的基于XML的文件格式**

- XLSX uses the open file format, which means it is generally accessible to most other applications

  **XLSX使用开放文件格式，可以被大多数其他应用程序访问**

- A secure file format as it cannot save malicious code

  **无法保存恶意代码，因此被认为是更安全的文件格式之一**



****

### Extensible Markup Language `.XML`

-  A  markup language with set rules for encoding data

  **一种标记语言，具有对数据进行编码的设定规则**

- Readable by both human and machine

  **可以被人类和机器读取**

- Self-descriptive language, design for sending messages through the internet

  **自我描述的语言，专为通过互联网发送信息而设计**

- Similar to HTML in some respects

- Does not predfined tags like Html does

- Platform independent

- Programming language independent

- **Makes it simpler to share data between systems**

****

### Portable Document Format `.PDF`

- File format developed by Adobe to present documents independent of application software, hardware, and operating systems

- Can be viewed the same way on any device

  **可以在任何设备上以相同的方式查看**

- Used in leagal and financial documents

  **用于法律和财务文件**

- Can also be used to fill in data for forms

****

### JavaScript Obejct Notation `.JSON`

- A text-based open standard designed for transmitting structured data over the web
- Language-independent data format
- **Can be read in any programming language**
- Easy to use
- Compataible with a wide range of browsers
- Considered as one of the **best tools for sharing data**



****

| Term                                                      | Definition                                                   | Video where the term is introduced    |
| --------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------- |
| Comma-separated values (CSV) / Tab-separated values (TSV) | Commonly used format for storing tabular data as plain text where either the comma or the tab separates each value. | Working on Different File Formats     |
| Data file types                                           | A computer file configuration is designed to store data in a specific way. | Working on Different File Formats     |
| Data format                                               | How data is encoded so it can be stored within a data file type. | Working on Different File Formats     |
| Data visualization                                        | A visual way, such as a graph, of representing data in a readily understandable way makes it easier to see trends in the data. | Data Science Topics and Algorithms    |
| Delimited text file                                       | A plain text file where a specific character separates the data values. | Working on Different File Formats     |
| Extensible Markup Language (XML)                          | A language designed to structure, store, and enable data exchange between various technologies. | Working on Different File Formats     |
| Hadoop                                                    | An open-source framework designed to store and process large datasets across clusters of computers. | What Makes Someone a Data Scientist   |
| JavaScript Object Notation (JSON)                         | A data format compatible with various programming languages for two applications to exchange structured data. | Working on Different File Formats     |
| Jupyter notebooks                                         | A computational environment that allows users to create and share documents containing code, equations, visualizations, and explanatory text. See Python notebooks. | Data Science Skills & Big Data        |
| Nearest neighbor                                          | A machine learning algorithm that predicts a target variable based on its similarity to other values in the dataset. | Working on Different File Formats     |
| Neural networks                                           | A computational model used in deep learning that mimics the structure and functioning of the human brain’s neural pathways. It takes an input, processes it using previous learning, and produces an output. | A Day in the Life of a Data Scientist |
| Pandas                                                    | An open-source Python library that provides tools for working with structured data is often used for data manipulation and analysis. | Data Science Skills & Big Data        |
| Python notebooks                                          | Also known as a “Jupyter” notebook, this computational environment allows users to create and share documents containing code, equations, visualizations, and explanatory text. | Data Science Skills & Big Data        |
| R                                                         | An open-source programming language used for statistical computing, data analysis, and data visualization. | Data Science Skills & Big Data        |
| Recommendation engine                                     | A computer program that analyzes user input, such as behaviors or preferences, and makes personalized recommendations based on that analysis. | A Day in the Life of a Data Scientist |
| Regression                                                | A statistical model that shows a relationship between one or more predictor variables with a response variable. | Data Science Topics and Algorithms    |
| Tabular data                                              | Data that is organized into rows and columns.                | A Day in the Life of a Data Scientist |
| XLSX                                                      | The Microsoft Excel spreadsheet file format.                 | Working on Different File Formats     |



****

# Cloud Computing 云计算



## Introduction

- It is the delivery of on-demand computing resources

  **通过互联网按需交付按需计算资源**

  - Networks
  - Servers
  - Storage
  - Applications
  - Services
  - Data centers



****

## User benefits

- No need to purchase applications and install them on local computer

  **不需要购买应用以及安装**

- Use online versions of applications and pay a monthly subscription

  **使用应用的网络版本，并支付每月订阅**

- More cost-effective

  **更具成本效益**

- Access most current software versions

- Save local storage space

  **节约本地存储空间**

- Work collaboratively in real time

  **和同事共同合作**





****

## Structure

- Five characteristics

  **五个基本特征**

  - On-demand self-service
  - Broad network access
  - Resource pooling
  - Rapid elasticity
  - Measured service

- Three deployment models

  **三种部署模型**

  - Public **公共云**
  - Private **私有云**
  - Hybird cloud **混合云**

- Three service models

  **三种服务模型**

  - Infrastructure

    **基础架构**

    - Infrastructure as a Service (Iaas)

      **无需管理或操作即可访问基础架构和物理计算仔元**

  - Platform

    **平台**

    - Platform as a Service(PaaS)

      **可以访问包含硬件和软件工具的平台**

  - Software

    **软件**

    - Software as a Service (SaaS)

      **软件许可和交付模式，软件和应用程序集中托管，并以订阅为基础进行许可**

    





****

# Big data processing technologies



## Apache Hadoop

- A collection of tools that provides distributed storage and processing of big data

  **可提供大数据的分布式存储和处理**

- A java-based open-source framework, allows distributed storage and processing of large datasets across clusters of computers

  **基于JAVA的开源框架，允许计算机集群分布式存储和处理大型数据集**

- A node is a single computer, and a collection of node forms a cluster![42733875cc18868650a1394e987ed6a](D:\JAVA\data scientist.assets\42733875cc18868650a1394e987ed6a.png)

- Scale up from a single node to any number of nodes, each offering local storage and computation

  **可以从单个节点扩展到任意数量的节点，每个节点都提供本地存储和计算**

- Provides a **reliable,scalable and cost-effective** solution for storing data with no format requirements

  **提供了一种可靠，可扩展且经济实惠的方案，用于在没有格式要求的情况下存储数据**

- Intended for long sequential scans

  **适用于长顺序扫描**



****

### Benifits of Hadoop

1. **Better real-time data driven decisions**

   - Incorporates emerging data formats not tradionally used in data warehouses

     **整合了传统上数据仓库中不适用的数据格式**

2. **Improved data access and analysis**

   - Provides real-time, self-service access to stakeholders

3. Data offload and consolidation

   **优化和简化企业数据仓库的成本**

4. Fast recovery from hardware failures

   **从硬件中快速恢复**

5. Access to streaming data

   **访问流数据**

6. Accommodation of large data sets

   **容纳大型数据集**

7. Portability

   **便携性**

****

### Hadoop main compoenets

1. Distributed File system (**HDFS**)

   - A storage system for big data that runs on multiple commodity hardware connected through a network

     **通过网络链接的多个商用硬件上运行的大数据存储系统**

   - Partitioning files over multiple nodes to provide scalable and reliable big data storage

     **将文件分区到多个节点来提供可扩展和可靠的大数据存储**

   - Spilt large file across multiple computers to allow parallel access to them

     **分散大文件到多个电脑上，并允许并行访问**

   - Replicates file blocks on different nodes to prevent data loss

     **复制不同节点上的文件来预防数据丢失**

****

## Hive

- A data warehouse for data query and anaysis

  **基于Hadoop构建的用于数据查询和分析的数据仓库**

- For reading, writing and managing large data set files that are stored directly in either **HDFS** or other data storage systmes such as **HBase**

   **Hive 是一款开源数据仓库软件，用于读取、写入和管理** 

  **直接存储在 HDFS 或其他数据存储系统（例如** 

  **Apache HBase）中的大型数据集文件。**

- **Disadvantage 缺点**

  - Based on **Hadoop**, so queries have high latency

    - **Not suitable for applications that need fast resopnse times**

    **不适用于快速响应的应用**

  - Based on read

    - **Not suitable for transaction processing that involves a high percentage of write operations**

      **不适用于涉及高比例写入操作的事务处理**

- **Advantage**

  - Suitable for data warehousing tasks such as ETL, reporting and data analysis

    **适合数据仓库等任务**

  - Easy access to data via **SQL**

****



## Spark

- A distributed analytics framework for complex, real-time data analytics

  **一个分布式数据分析框架，旨在实时执行复杂的数据分析**

- A general-purpose data processing engine designed to extract and process large volumes of data for a wide range of applications

  **旨在为各种应用提取和处理大量数据**

  - Interactive Analytics 
  - Streams Processing
  - Machine learning
  - Data Integration
  - ETL

****

### Key attributes

- Has in-memory processing which increases speed of computations

  **利用内存处理来显著提高计算速度**

- Splling to disk only when memory is constrained

  **只有当存储受限才会溢出到磁盘**

- Can run using its standalong clustering technology as well as on top of other infrastructures (Hadoop)

  **可以使用独立的群集技术运行,也可以在其他基础架构上运行**

- Access data in a large variety of data sources, incuding **HDFS AND HIVE**

  **可以访问各种数据源中的数据**

- **Key use case**

  - **Process streaming data fast and perform complex analytics in real-time**

    **快速处理流数据和实时执行复杂分析的能力** 

    **是 Apache Spark 的关键用例**



****

# 数据挖掘

### 1. 确立数据挖掘目标

- **设定明确的目标**：明确需要回答的关键问题。
- **考虑成本和收益**：评估数据挖掘的成本效益，确保投入与预期收益相符。
- **确定准确性和有用性**：提前设定结果的预期准确度和实用性水平。
- **权衡取舍**：高精度结果可能增加成本，需考虑边际收益递减的影响。

### 2. 选择数据

- **数据质量至关重要**：结果的有效性取决于所用数据的质量。
- **评估数据可用性**：确定现有数据是否足够，或者是否需要收集新数据（如开展调查）。
- **控制数据收集成本**：数据类型、规模和收集频率直接影响项目成本。

### 3. 数据预处理

- **清理数据**：识别并删除错误或不相关的数据。
- **处理缺失值**：确定缺失数据是随机的还是系统性的，并制定处理策略。
- **确保数据完整性**：检查并纠正数据中的错误，如错误合并或解析。

### 4. 数据转换

- **优化数据格式**：将数据转换为适合存储和分析的格式。
- **减少维度**：使用降维技术（如主成分分析）减少属性数量，降低复杂性。
- **变量转换**：根据需要转换变量类型（如将连续型转为分类型）以捕捉非线性关系。

### 5. 数据存储

- **高效存储**：以便于数据挖掘的方式存储数据，确保快速读写访问。
- **数据安全和隐私**：保护数据安全，防止未经授权的访问和数据分散。
- **支持动态更新**：允许在数据挖掘过程中生成的新变量写回数据库。

### 6. 数据挖掘

- **应用分析方法**：使用统计和机器学习算法对数据进行深入分析。
- **数据可视化**：利用高级图形功能探索数据趋势，建立初步理解。
- **迭代分析**：根据初步结果不断调整和优化分析方法。

### 7. 评估挖掘结果

- **验证模型性能**：通过样本内预测测试模型的有效性和效率。
- **反馈与改进**：将结果与利益相关者分享，收集反馈以改进分析。
- **持续优化**：将数据挖掘和评估过程迭代化，持续提升结果质量。



****

| Term                                  | Definition                                                   | Video where the term is introduced                       |
| ------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------------- |
| Analytics                             | The process of examining data to draw conclusions and make informed decisions is a fundamental aspect of data science, involving statistical analysis and data-driven insights. | Data Scientists at New York University                   |
| Big Data                              | Vast amounts of structured, semi-structured, and unstructured data are characterized by its volume, velocity, variety, and value, which, when analyzed, can provide competitive advantages and drive digital transformations. | How Big Data is Driving Digital Transformation           |
| Big Data Cluster                      | A distributed computing environment comprising thousands or tens of thousands of interconnected computers that collectively store and process large datasets. | What is Hadoop?                                          |
| Broad Network Access                  | The ability to access cloud resources via standard mechanisms and platforms such as mobile devices, laptops, and workstations over networks. | Introduction to Cloud                                    |
| Chief Data Officer (CDO)              | An emerging role responsible for overseeing data-related initiatives, governance, and strategies, ensuring that data plays a central role in digital transformation efforts. | How Big Data is Driving Digital Transformation           |
| Chief Information Officer (CIO)       | An executive is responsible for managing an organization's information technology and computer systems, contributing to technology-related aspects of digital transformation. | How Big Data is Driving Digital Transformation           |
| Cloud Computing                       | The delivery of on-demand computing resources, including networks, servers, storage, applications, services, and data centers, over the Internet on a pay-for-use basis. | Introduction to Cloud                                    |
| Cloud Deployment Models               | Categories that indicate where cloud infrastructure resides, who manages it, and how cloud resources and services are made available to users, including public, private, and hybrid models. | Introduction to Cloud                                    |
| Cloud Service Models                  | Models based on the layers of a computing stack, including Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS), represent different cloud computing offerings. | Introduction to Cloud                                    |
| Commodity Hardware                    | Standard, off-the-shelf hardware components are used in a big data cluster, offering cost-effective solutions for storage and processing without relying on specialized hardware. | What is Hadoop?                                          |
| Data Algorithms                       | Computational procedures and mathematical models used to process and analyze data made accessible in the cloud for data scientists to deploy on large datasets efficiently. | Cloud for Data Science                                   |
| Data Replication                      | A strategy in which data is duplicated across multiple nodes in a cluster to ensure data durability and availability, reducing the risk of data loss due to hardware failures. | What is Hadoop?                                          |
| Data Science                          | An interdisciplinary field that involves extracting insights and knowledge from data using various techniques, including programming, statistics, and analytical tools. | Data Scientists at New York University                   |
| Deep Learning                         | A subset of machine learning that involves artificial neural networks inspired by the human brain, capable of learning and making complex decisions from data on their own. | Data Scientists at New York University                   |
| Digital Change                        | The integration of digital technology into business processes and operations leads to improvements and innovations in how organizations operate and deliver value to customers. | How Big Data is Driving Digital Transformation           |
| Digital Transformation                | A strategic and cultural organizational change driven by data science, especially Big Data, to integrate digital technology across all areas of the organization, resulting in fundamental operational and value delivery changes. | How Big Data is Driving Digital Transformation           |
| Distributed Data                      | The practice of dividing data into smaller chunks and distributing them across multiple computers within a cluster enables parallel processing for data analysis. | What is Hadoop?                                          |
| Hadoop                                | A distributed storage and processing framework used for handling and analyzing large datasets, particularly well-suited for big data analytics and data science applications. | Data Scientists at New York University                   |
| Hadoop Distributed File System (HDFS) | A storage system within the Hadoop framework that partitions and distributes files across multiple nodes, facilitating parallel data access and fault tolerance. | What is Hadoop?                                          |
| Infrastructure as a Service (IaaS)    | A cloud service model that provides access to computing infrastructure, including servers, storage, and networking, without the need for users to manage or operate them. | Introduction to Cloud                                    |
| Java-Based Framework                  | Hadoop is implemented in Java, an open-source, high-level programming language, providing the foundation for building distributed storage and processing solutions. | Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark |
| Map Process                           | The initial step in Hadoop’s MapReduce programming model, where data is processed in parallel on individual cluster nodes, often used for data transformation tasks. | What is Hadoop?                                          |
| Measured Service                      | A characteristic where users are billed for cloud resources based on their actual usage, with resource utilization transparently monitored, measured, and reported. | Introduction to Cloud                                    |
| On-Demand Self-Service                | The capability for users to access and provision cloud resources such as processing power, storage, and networking using simple interfaces without human interaction with service providers. | Introduction to Cloud                                    |
| Rapid Elasticity                      | The ability to quickly scale cloud resources up or down based on demand, allowing users to access more resources when needed and release them when not in use. | Introduction to Cloud                                    |
| Reduce Process                        | The second step in Hadoop's MapReduce model is where results from the mapping process are aggregated and processed further to produce the final output, typically used for analysis. | What is Hadoop?                                          |
| Replication                           | The act of creating copies of data pieces within a big data cluster enhances fault tolerance and ensures data availability in case of hardware or node failures. | What is Hadoop?                                          |
| Resource Pooling                      | A cloud characteristic where computing resources are shared and dynamically assigned to multiple consumers, promoting economies of scale and cost-efficiency. | Introduction to Cloud                                    |
| Skills Network Labs (SN Labs)         | Learning resources provided by IBM, including tools like Jupyter Notebooks and Spark clusters, are available to learners for cloud data science projects and skill development. | Cloud for Data Science                                   |
| Spilling to Disk                      | A technique used in memory-constrained situations where data is temporarily written to disk storage when memory resources are exhausted, ensuring uninterrupted processing. | Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark |
| STEM Classes                          | Science, Technology, Engineering, and Mathematics (STEM) courses typically taught in high schools prepare students for technical careers, including data science. | Data Scientists at New York University                   |
| Variety                               | The diversity of data types, including structured and unstructured data from various sources such as text, images, video, and more, posing data management challenges. | Foundations of Big Data                                  |
| Velocity                              | The speed at which data accumulates and is generated, often in real-time or near-real-time, drives the need for rapid data processing and analytics. | Foundations of Big Data                                  |
| Veracity                              | The quality and accuracy of data, ensuring that it conforms to facts and is consistent, complete, and free from ambiguity, impacts data reliability and trustworthiness. | Foundations of Big Data                                  |
| Video Tracking System                 | A system used to capture and analyze video data from games, enabling in-depth analysis of player movements and game dynamics, contributing to data-driven decision-making in sports. | How Big Data is Driving Digital Transformation           |
| Volume                                | The scale of data generated and stored is driven by increased data sources, higher-resolution sensors, and scalable infrastructure. | Foundations of Big Data                                  |
| V's of Big Data                       | A set of characteristics common across Big Data definitions, including **Velocity, Volume, Variety, Veracity, and Value**, highlighting the rapid generation, scale, diversity, quality, and value of data. | Foundations of Big Data                                  |



****

# AI and DS

- Big data **大数据**

  - refers that data sets are so massive, so quickly built and so varied that they defy traditional analysis methods.

    **指的是如此庞大，构建速度如此之快，变化如此之大的数据集，以至于违背了传统的分析方法**

  - Described in 5 V's

    - Velocity

      **速度**

    - Volume

      **体积**

    - Variety

      **多样性**

    - Veracity

      **真实性**

    - Value

      **价值**

  

- **Data mining 数据挖掘**

  - Automatically searching and analyzing data to discover previously unrevealed patterns

    **自动搜索和分析数据来发现之前未被揭露的过程**

****

# Generative AI and DS

- Focusing on produces new data

  **专注于产生新的数据而不仅仅是分析数据**

  - Creates content
    - Images
    - Music
    - Language
    - Code

- Working foundations (Deep learning) **工作的基础**

  - GAN **生成对抗网络**
  - Variational auto-encoders (VAEs)

  

  | Term                              | Definition                                                   | Video where the term is introduced       |
  | --------------------------------- | ------------------------------------------------------------ | ---------------------------------------- |
  | Artificial Neural Networks        | Collections of small computing units (neurons) that process data and learn to make decisions over time. | Artificial Intelligence and Data Science |
  | Bayesian Analysis                 | A statistical technique that uses Bayes' theorem to update probabilities based on new evidence. | Applications of Machine Learning         |
  | Business Insights                 | Accurate insights and reports generated by generative AI can be updated as data evolves, enhancing decision-making and uncovering hidden patterns. | Generative AI and Data Science           |
  | Cluster Analysis                  | The process of grouping similar data points together based on certain features or attributes. | Neural Networks and Deep Learning        |
  | Coding Automation                 | Using generative AI to automatically generate and test software code for constructing analytical models, freeing data scientists to focus on higher-level tasks. | Generative AI and Data Science           |
  | Data Mining                       | The process of automatically searching and analyzing data to discover patterns and insights that were previously unknown. | Artificial Intelligence and Data Science |
  | Decision Trees                    | A type of machine learning algorithm used for decision-making by creating a tree-like structure of decisions. | Applications of Machine Learning         |
  | Deep Learning Models              | Includes Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) that create new data instances by learning patterns from large datasets. | Generative AI and Data Science           |
  | Five V's of Big Data              | Characteristics used to describe big data: Velocity, volume, variety, veracity, and value. | Neural Networks and Deep Learning        |
  | Generative AI                     | A subset of AI that focuses on creating new data, such as images, music, text, or code, rather than just analyzing existing data. | Generative AI and Data Science           |
  | Market Basket Analysis            | Analyzing which goods tend to be bought together is often used for marketing insights. | Neural Networks and Deep Learning        |
  | Naive Bayes                       | A simple probabilistic classification algorithm based on Bayes' theorem. | Applications of Machine Learning         |
  | Natural Language Processing (NLP) | A field of AI that enables machines to understand, generate, and interact with human language, revolutionizing content creation and chatbots. | Generative AI and Data Science           |
  | Precision vs. Recall              | Metrics are used to evaluate the performance of classification models. | Applications of Machine Learning         |
  | Predictive Analytics              | Using machine learning techniques to predict future outcomes or events. | Neural Networks and Deep Learning        |
  | Synthetic Data                    | Artificially generated data with properties similar to real data, used by data scientists to augment their datasets and improve model training. | Generative AI and Data Science           |

- Big Data 具有五个特征：速度、数量、种类、真实性和价值。

- Cloud Computing 的五个特征是：按需自助服务、广泛的 Networking、资源池、快速弹性和计量服务。 

- 数据 Mining 有六个步骤：设定目标、选择数据源、预处理、Transformer、挖掘和评估。

- 有了这么多由人员、工具和机器创建的不同数量的数据，就需要新的、创新的、可扩展的技术来推动 Transformer。

- 深度学习利用 Neural Network 自学输入和输出中的模式。Machine Learning 是 AI 的一个子集，它使用计算机算法来学习数据并进行预测，而无需将分析方法明确编程到系统中。

- Regression 可确定一个或多个输入与输出之间相关性的强度和数量。

- 处理 Big Data 所涉及的技能包括应用统计学、Machine Learning 模型和一些计算机编程。

- Generative AI 是人工智能的一个子集，侧重于生产新数据，而不仅仅是分析现有数据。它允许机器模仿人的创作来创造内容，包括图像、音乐、语言、计算机代码等。

****

| Term                 | Definition                                                   | Video where the term is introduced       |
| -------------------- | ------------------------------------------------------------ | ---------------------------------------- |
| Arithmetic Models    | Data science often uses Mathematical models to analyze data and predict outcomes. | Old problems, new data science solutions |
| Case study           | In-depth analysis of an instance of a chosen subject to draw insights that inform theory, practice, or decision-making. | Old problems, new data science solutions |
| Data mining          | Extracting information from raw data, such as making decisions, predicting trends, or understanding phenomena. | How Data Science is Saving Lives         |
| Data Science         | The field involves collecting, analyzing, and interpreting data to extract valuable insights and make informed decisions. | Old problems, new data science solutions |
| Data Strategy        | A plan that outlines how an organization will collect, manage, and use data to achieve its goals. | Old problems, new data science solutions |
| Predictive analytics | Using data, algorithms, models, and machine learning to make predictions. | How Data Science is Saving Lives         |





****

| Term                                    | Definition                                                   | Video where the term is introduced                        |
| --------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------- |
| Adobe Spark                             | A suite of software tools that allow users to create and share visual content such as graphics, web pages, and videos. | Recruiting for Data Science                               |
| Analytical skills                       | The ability to analyze information systematically, logically, and organized. | Recruiting for Data Science                               |
| Chief information officer (CIO)         | A business executive is responsible for an organization’s information technology systems and tech-related initiatives. | How Can Someone Become a Data Scientist                   |
| Computational thinking                  | Breaking problems into smaller parts and using algorithms, logic, and abstraction to develop solutions. Often used but not limited to computer science. | How Can Someone Become a Data Scientist                   |
| Data clusters                           | A group of similar, related data points distinct from other clusters. | How Can Someone Become a Data Scientist                   |
| Executive summary                       | Usually occurring at the beginning of a research paper, this section summarizes the important parts of the paper, including its key findings. | The Report Structure                                      |
| High-performing computing (HPC) cluster | A computing technology that uses a system of networked computers designed to solve complex and computationally intensive problems in traditional environments. | How Can Someone Become a Data Scientist                   |
| Mathematical computing                  | The use of computers to calculate, simulate, and model mathematical problems. | Importance of Mathematics and Statistics for Data Science |
| Matrices                                | Plural for matric, matrices are a rectangular (tabular) array of numbers often used in mathematics, statistics, and computer science. | Recruiting for Data Science                               |
| Stata                                   | A software package used for statistical analysis.            | Recruiting for Data Science                               |
| Statistical distributions               | A way of describing the likelihood of different outcomes based on a dataset. The “bell curve” is a common statistical distribution. | How Can Someone Become a Data Scientist                   |
| Structured Query Language (SQL)         | A language used for managing data in a relational database.  | Importance of Mathematics and Statistics for Data Science |
| TCP/IP network                          | A network that uses the TCP/IP protocol to communicate between connected devices on that network. The Internet uses TCP/IP. | How Can Someone Become a Data Scientist                   |

- 数据科学可以帮助医生为病人提供最佳治疗，帮助气象学家预测当地天气事件的范围，甚至可以帮助预测地震和龙卷风等自然灾害。
- 公司可以通过采集数据开始数据科学之旅。一旦掌握了数据，就可以开始对其进行分析。
- 每个使用互联网的人每天都会产生大量数据。
- 亚马逊和 Netflix 使用推荐引擎，UPS 使用来自客户、司机和车辆的数据来有效利用司机的时间和燃料。
- 数据科学项目最终成果的目的是将数据分析中获得的新信息和见解传达给关键决策者。
- 报告应全面分析数据并传达项目发现。
- 公司应寻找对其特定行业的数据工作感到兴奋的人。他们应该寻找有好奇心的人，能够就他们打算收集的数据类型提出有趣、有意义的问题。他们应该聘用那些热爱数据工作、精通统计学并能熟练应用机器学习算法的人。
- 一份条理清晰、逻辑严密的报告应向读者传达以下信息：
  - 读者阅读报告的收获
  - 明确的目标
  - 您所做贡献的意义
  - 通过提供充分的背景资料来介绍适当的背景 
  - 为什么这项工作切实有用
  - 猜想您的工作可能带来的可信的未来发展  

![img](D:\JAVA\data scientist.assets\60de3juKS3G57hPM_kAnYw_97f87cb97da24f71b0db78f9935a8ef1_200457.088-Infograph-on-roadmap-v2.png)





****

# 求职经历



### 1. 教育和技能获取 (Education and Skill Acquisition)

- Lila 拥有经济学学士学位和数据分析背景，决定转型为数据科学家。
- 她通过 IBM 数据科学专业证书课程学习统计、机器学习、数据分析和编程语言（如 Python 和 SQL）。

### 2. 打好基础 (Building a Strong Foundation)

- Lila 掌握了数据操作和可视化的基础知识，尤其是通过 Python 库（如 NumPy、Pandas 和 Matplotlib），为数据分析奠定了坚实的基础。

### 3. 通过可视化讲故事 (Visualization for Storytelling)

- Lila 学习如何通过数据可视化工具来有效传达她的分析结果，帮助利益相关者理解数据背后的意义。

### 4. 实践经验 (Hands-On Experience)

- Lila 通过 Kaggle 比赛和个人数据项目获取实践经验，增强了她的实际数据问题解决能力，并通过 GitHub 展示她的项目。

### 5. 数据整理与预处理 (Data Wrangling and Preprocessing)

- Lila 学会了使用 NumPy 和 Pandas 进行数据清理和预处理，处理缺失数据、异常值，并进行特征工程以提高模型性能。

### 6. 沟通与数据讲述 (Communication and Storytelling)

- 她通过学习 Matplotlib 和 Plotly 等工具，提高了数据讲述的能力，能够清晰有效地展示数据见解。

### 7. 网络与合作 (Networking and Collaboration)

- Lila 积极参与数据科学社区，参加会议，参与开源项目，扩展了她的专业网络，并从行业专家中获得启发。

### 8. 行业领域专长 (Domain Expertise)

- Lila 深入探索了电商、医疗、金融等领域，并选择电商作为她的核心领域，结合她的经济学背景，应用数据科学技能。

### 9. 获得第一份工作 (Landing the First Job)

- 准备多月后，Lila 开始申请数据科学职位，调整简历以突出相关技能，并展示在线作品集来展示她的能力。

### 10. 数据科学家的首个任务 (Lila's Approach to Working on Her First Task as a Data Scientist)

- 在零售公司，Lila 通过分析客户数据，找出模式和异常，改进客户服务并增强客户体验。

### 11. 数据集选择与来源 (Dataset Selection and Sourcing)

- Lila 面对数据选择的挑战，从多个来源收集数据，并与产品专家和数据工程师合作，整合多个数据集。

### 12. 数据理解与清理 (Data Understanding and Cleaning)

- Lila 使用 Python 和 SQL 导入数据并清理数据，解决了数据中的缺失值、重复值和异常值问题。

### 13. 探索性数据分析 (Exploratory Data Analysis - EDA)

- Lila 通过 EDA 生成摘要统计和可视化图表，深入了解客户行为、产品流行度和销售趋势。

### 14. 特征工程 (Feature Engineering)

- 她通过创建新的特征（如总购买金额）来增强分析效果，提高模型的准确性。

### 15. 统计分析与机器学习 (Statistical Analysis, Machine Learning)

- Lila 进行了回归分析和机器学习模型的探索，用于需求预测和客户细分。

### 16. 演示与报告 (Presentation and Reporting)

- 她将分析结果编写成报告并通过 Jupyter Notebook 向利益相关者展示，提供可操作的见解和建议。

### 17. 持续学习 (Continuous Learning)

- Lila 完成第一个项目后继续学习，探索更复杂的数据集，并挑战更具难度的数据科学任务。

### 18. 机器学习技能 (Machine Learning Skills)

- Lila 对机器学习产生浓厚兴趣，通过 IBM 机器学习专业证书进一步提升自己的技能，深入研究线性回归、决策树和深度学习模型。



****

# 用于数据科学的Opensoure工具

## Objective

This reading provides a summary of key open-source tools for Data Science covered in the Part 1 and Part 2 videos of this course.

They are broadly classified as -

- **Data Management Tools** - Facilitates the storage, organization, and retrieval of data. Includes Relational Databases, NoSQL Databases, and Big Data platforms.
- **Data Integration and Transformation Tools** - Streamlines data pipelines and automate data processing workflows. Task of data integration and transformation in the classic data warehousing world is to Extract, Transform, and Load (ETL).
- **Data Visualization Tools**- Provides graphical representation of data and assist with communicating insights.
- **Model Deployment, Monitoring and Assessment Tools**- Supports the building, deploying, monitoring, and evaluation of data and machine learning models.
- **Data Asset Management Tools**- Organizes and manages data, enforce access controls, and ensure asset backups.
- **Code Development and Execution Tools** - ProvideS environments for developing, testing, and deploying code, offering computational resources to execute it.
- **Code Asset Management Tools** - Enables the storage and management of code, track changes, and support collaborative development.

## Data Management Tools

### MySQL

- Popular open source **relational database management system** (RDBMS)
- Uses structured query language (SQL) to manage and store data.
- Common uses:
  - Web applications
  - Data warehousing
  - E-commerce

### PostgreSQL

- Powerful and open source **relational database management system** (RDBMS)
- Emphasizes extensibility and SQL compliance.
- Offers advanced features such as:
  - Support for JSON
  - Full-text search
  - Spatial data

### Apache CouchDB

- Document-oriented **NoSQL** database
- Uses JSON to store data
- Highly scalable
- Fault-tolerant
- Easy to use

### MongoDB

- Document-oriented **NoSQL** database
- Stores data in a flexible JSON
- Provides:
  - Scalability
  - High availability
  - Data distribution
- Suitable for modern web applications that handle large volumes of unstructured data

### Apache Cassandra

- Highly scalable, distributed Document-oriented **NoSQL** database
- Can handle large amounts of structured and unstructured data across many commodity servers.
- Offers:
  - High availability
  - Fault tolerance
  - Tunable consistency levels
- Suitable for mission-critical applications

### Hadoop Distributed File System (HDFS)

- Designed to work with large datasets like Apache Hadoop in a distributed computing environment
- High-throughput data processing by splitting files into blocks (default 128MB), and these blocks are distributed across multiple DataNodes
- Data is replicated across different DataNodes ensuring high availability and fault tolerance
- Scalable and efficient

### Ceph

- Free, open source software-defined storage platform suitable for hybrid cloud environments
- Designed for modern data centers
- Provides highly scalable, unified storage system that can be used for object storage (like AWS S3), block storage (like virtual disks for VMs), and file storage (like NFS) under one unified system
- High performance, availability and reliability

### Elasticsearch

- Primarily a distributed RESTful search engine and analytics tool
- Based on the Lucene library.
- Full-text search, real-time data analytics
- Highly scalable
- Easy to use
- Powerful querying capabilities
- Real-time data indexing for fast document retrieval.

## Data Integration and Transformation Tools

### Apache Airflow

- Open-source platform for programmatically authoring, scheduling, and monitoring workflows
- Created originally by Airbnb
- Allows users to define and execute complex workflows
- Support for:
  - Task dependencies
  - Parallelism
  - Error handling

### Kubeflow

- An open-source machine learning toolkit that allows execution of data science pipelines on top of Kubernetes.
- Provides a platform for building, deploying, and managing end-to-end machine learning workflows at scale
- Support for:
  - Distributed training
  - Model serving
  - Hyperparameter tuning

### Apache Kafka

- Distributed streaming platform that allows applications to publish, process, and subscribe to streams of records in real-time
- Created originally from LinkedIn.
- It is scalable, fault-tolerant, and high-throughput
- Suitable for building mission-critical, data-intensive applications

### Apache NiFi

- An open-source data integration platform that allows users to automate the flow of data between systems
- Provides a web-based user interface for designing and managing data flows
- Support for:
  - Data routing
  - Transformation
  - Enrichment
  - Among other capabilities

### Apache Spark SQL

- A module in the Spark ecosystem that provides a programming interface for working with structured data using:
  - SQL
  - Data frames
  - Datasets
- Supports a wide range of data sources and provides optimized performance for complex data processing tasks.

### Node-RED

- An open-source visual programming tool for wiring together hardware devices, APIs, and online services
- Allows users to create event-driven flows of messages
- low in resource consumption that it even runs on tiny devices like a Raspberry Pi.
- Support for:
  - Data transformation
  - Filtering
  - Aggregation

## Data Visualization Tools

### PixieDust

- Open-source library for creating interactive, exploratory data visualizations in Python and Jupyter notebooks
- Provides a range of built-in visualizations and data connectors
- Support for customization and extensibility through third-party libraries

### Hue

- Open-source web interface for analyzing and visualizing large datasets in Apache Hadoop
- Offers a user-friendly experience for exploring data and creating visualizations
- No need for programming skills; can create visualizations from SQL queries

### Kibana

- Open-source data visualization tool that allows users to interact with their data through a web-based interface
- Commonly used with Elasticsearch to analyze and visualize large datasets

### Apache Superset

- A modern, enterprise-ready business intelligence web application that makes it easy to visualize and explore large datasets
- Offers a rich set of data visualization options, including:
  - Charts
  - Tables
  - Maps
  - Geospatial analysis
  - Real-time data processing

## Model Deployment Tools

### Apache PredictionIO

- Open-source machine learning server built on a scalable and distributed infrastructure
- Allows developers to quickly build, evaluate, and deploy predictive engines for various use cases such as:
  - Recommendation
  - Classification
  - Clustering

### Kubernetes

- Open-source platform for container orchestration
- Automatically launches, scales, and manages containerized applications
- Offering features like:
  - Automatic scaling
  - Self-healing
  - Load balancing
- Enables the management and orchestration of containers across numerous hosts

### MLeap

- Open-source library for serializing and deserializing learning models in a cross-platform file
- Gives users the ability to export models from different machine learning libraries and frameworks, such as:
  - Spark
  - Scikit-learn
  - TensorFlow
- Implements them in high-throughput, low-latency production environments

### TensorFlow Lite

- Open-source tool for running machine learning models on mobile and embedded devices
- Allows effective inference on mobile and embedded platforms
- Supports a variety of hardware accelerators such as:
  - CPUs
  - GPUs
  - Custom ASICs

### Apache Seldon

- Open-source platform for deploying and managing machine learning models on Kubernetes
- Provides a way to:
  - Serve models at scale
  - Automate model deployment workflows
  - Monitor the performance of deployed models in real-time

### Red Hat OpenShift

- Container application framework based on Kubernetes
- With characteristics like automation, scalability, and security
- Offers a method for creating, deploying, and managing containerized applications

### TensorFlow Serving

- Open-source utility that serves machine learning models in real-world settings
- Supports both HTTP and gRPC interfaces for serving predictions
- Provides high scalability and low latency deployment and management of TensorFlow models

### TensorFlow.js

- Open-source library for building and deploying machine learning models in JavaScript
- Allows you to train and execute models directly in the browser or on Node.js
- Supports a wide range of model architectures, including neural networks, decision trees, and k-nearest neighbors

## Model Monitoring and Assessment Tools

### IBM AI Fairness 360

- Open-source toolkit for detecting and mitigating bias in machine learning models
- Provides a way to measure the fairness and bias of models, as well as a set of algorithms for mitigating bias and creating fairer models

### IBM AI Explainability 360

- Open-source toolkit for explaining the behavior and decisions of machine learning models
- Provides a way to measure the explainability and interpretability of models, as well as a set of algorithms for generating explanations and visualizations of model behavior

### The IBM Adversarial Robustness 360 Toolbox

- Free and open-source library for protecting machine learning models from adversarial attacks
- Includes a method for measuring model robustness and vulnerability
- Includes a set of algorithms for improving model robustness and detecting adversarial examples

### Prometheus

- Freely available monitoring system that collects and stores metrics in real-time from different sources
- Allows you to visualize and set alerts on the health and performance of systems and apps
- Supports a variety of data gathering methods, such as HTTP endpoints, exporters, and agents

### ModelDB

- Open-source platform for managing machine learning models and experiments
- Provides a way to track and reproduce experiments, version models, and collaborate with team members

## Code Development and Execution Tools

### Jupyter IDE

- Open-source effort
- Supports:
  - Julia
  - Python
  - R development with Jupyter Notebook
  - JupyterLab
  - JupyterHub
- Create and share documents containing:
  - Live code
  - Equations
  - Visualizations
  - Narrative text
- JupyterLab includes:
  - Customized notebook organization
- JupyterHub extends all these capabilities to the enterprise

### RStudio

- For developers
- Free and open-source IDE
- Built to manage and execute R code
- Works on all platforms
- Includes:
  - Version control
  - Project management capabilities

### Microsoft Visual Studio

- An IDE that supports a variety of programming languages, including:
  - C
  - C++
  - C++/CLI
  - Visual Basic.NET
  - C#
  - F#
  - JavaScript
  - TypeScript
  - XML
  - XSLT
  - HTML
  - CSS
- Using plug-ins, supports:
  - Python
  - Ruby
  - Node.js
  - M
  - Other languages

### PyCharm

- Primarily a subscription-based IDE environment
- Offers 16+ additional tools for coding assistance, testing, and web development
- Supports scientific development with IPython integration and Matplotlib and NumPy support
- Also offers a free community-based, open-source IDE with limited capabilities

### Spyder

- Free, open-source Python-based IDE designed by and for scientists, engineers, and data analysts
- Features a unique combination of comprehensive development tools for:
  - Advanced editing
  - Analysis
  - Debugging
  - Profiling
  - Visualization capabilities

### Anaconda Navigator

- Open-source GUI-based Navigator that supports Python development and integrates with:
  - Eclipse and PyDev
  - IDLE
  - IntelliJ
  - Microsoft Visual Studio Code (VS Code)
  - Ninja IDE
  - PyCharm
  - Python for Visual Studio Code
  - Python Tools for Visual Studio (PTVS)
  - Spyder
  - Sublime Text
  - Wing IDE

## Code Asset Management Tools

### Git

- Open-source version control system for tracking changes in code and collaboration among developers
- Provides a way to manage and organize code changes, collaborate on code development, and maintain a history of code revisions

### GitLab

- Web-based Git repository manager
- Provides a complete DevOps platform for:
  - Source code management
  - Continuous integration and deployment
  - Monitoring
- Enables teams to collaborate on:
  - Code development
  - Automate build and deployment processes
  - Track metrics and performance across the entire software development lifecycle

### GitHub

- Web-based Git repository hosting service that provides a platform for developers to collaborate on code and manage software projects
- Enables users to:
  - Create, fork, and contribute to open source projects
  - Track changes in code
  - Manage issues
  - Pull requests

### Bitbucket from Atlassian

- Web-based Git repository hosting service
- Provides a platform for developers to collaborate on code and manage software projects, with features like:
  - Pull requests
  - Code review
  - Branch permissions



****

# Rstudio中的绘图

```R
install.packages<package name>
```

- `ggplot`: Histograms, bar charts, scatterplots

  **用于数据可视化，允许向单个可视化中添加图层和组件**

- `Plotly`: Web-based data visualizations

  **用于基本Web的数据可视化，可以显示或保存为单个HTML 文件**

- `Lattice`: Complex, multi-variable data sets

  **实现复杂的多变量数据集，高级可视化库，无需自定义即可处理图形**

- `Leaflet`: Intercative plots

  **用于创建交互式绘图**



****

# Github

- “**git add**” 将更改从工作目录移动到暂存区域。 

- **“git status**” 允许您查看工作目录的状态和 更改的暂存快照。 

- “**git commit**” 获取您分阶段的更改快照并将其提交到项目中。 

- **“git reset**” 会撤消你对工作目录中文件所做的更改。 

- **“git log**” 使您可以浏览以前对项目所做的更改。 

- **“git branch”** 允许您在存储库中创建一个隔离的环境来进行更改。 

- “**git checkout”** 允许你查看和更改现有分支。 

- **“git merge”** 可以让你把所有东西重新组合在一起。
